{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetworks, Main Infomation\n",
    "With back prop, we are normally dealing iwth numbers that are below zero or one\n",
    ", there isnt much towards\n",
    "that factor when we do that data to deal with , the thing that oyu would have to\n",
    "note is that, there are two\n",
    "passes that ococur when ever we do find the dx and dy function of everythign and\n",
    "for each given pass, and that\n",
    "is very important to point out\n",
    "\n",
    "Here is some following code that will state how thing would work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import nnfs\n",
    "import numpy as np\n",
    "from frosch import hook\n",
    "from nnfs.datasets import spiral_data\n",
    "from pprintpp import pprint as pp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this i will have to explain each given part of the code, in this case we are\n",
    "just doing a few inputs, the first one that is quite important would be the\n",
    "nnfs, this is the dataset that im learning off, and quite good one too, and also\n",
    "important as well .\n",
    "if you have a look at the code below i have explained all teh given features for\n",
    "why this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs: int, n_neurons: int):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        # This is just random generated results\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # If it is dying, but you can change that to a non zero\n",
    "\n",
    "    def forward(self, inputs: list) -> np:\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        self.dinputs = np.dot(dvalues, self.dweights.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With teh code above, you have the weights and the bias, in this case you know\n",
    "what these two are doing, its just making a dense layer, which is quite\n",
    "straightforwad, but then you have the self.inputs - this is making a copy of teh\n",
    "given inputs for future use, the reason for this is that for the inputs, wew ill\n",
    "ahve to make a copy out of it, the backward pass is the first step, in this case\n",
    "what we are doing , is finding the partial dx of weights and the partial dx of\n",
    "bias ,\n",
    "this is show below :\n",
    "\n",
    "The forward propagation equations are as follows:\n",
    "Input=x0Hidden Layer1 output=x1=f1(W1x0)Hidden Layer2\n",
    "output=x2=f2(W2x1)Output=x3=f3(W3x2)\n",
    "\n",
    "what we are doing is going from left to right, in regards to the prop, when we\n",
    "are doing the back propagation, which is an important point ot note ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "class ActivationReLU:\n",
    "    def forward(self, inputs: list) -> np:\n",
    "        self.outputs = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "class ActivationSoftMax:\n",
    "    def forward(self, inputs: list) -> np:\n",
    "        expVal = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Cool this works - Take Value away and keeep that dimension .\n",
    "        # This prevents overflow error\n",
    "\n",
    "        probability = expVal / np.sum(expVal, axis=1, keepdims=True)\n",
    "\n",
    "        self.outputs = probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # This function above would calculate the loss\n",
    "\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred: list, y_true: list) -> np:\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-07, 1 - 1e-07)\n",
    "\n",
    "        # Looking at the target values\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidence = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidence = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        cross_entropy = -np.log(correct_confidence)\n",
    "        return cross_entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X, y = spiral_data(samples=100, classes=3)  # This is your data\n",
    "\n",
    "dense1 = DenseLayer(2, 10)  # So inputs are 2\n",
    "dense2 = DenseLayer(10, 5)\n",
    "dense3 = DenseLayer(5, 10)\n",
    "dense4 = DenseLayer(10, 3)\n",
    "# ^ Because classes is three\n",
    "activation1 = ActivationReLU()\n",
    "activation2 = ActivationReLU()\n",
    "activation3 = ActivationReLU()\n",
    "activation4 = ActivationSoftMax()\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.outputs)\n",
    "\n",
    "dense2.forward(activation1.outputs)\n",
    "activation2.forward(dense2.outputs)\n",
    "\n",
    "dense3.forward(activation2.outputs)\n",
    "activation3.forward(dense3.outputs)\n",
    "\n",
    "dense4.forward(activation3.outputs)\n",
    "activation4.forward(dense4.outputs)\n",
    "\n",
    "loss = loss_function.calculate(activation4.outputs, y)\n",
    "print(\"Loss : \", loss)\n",
    "\n",
    "# This just compares those values with argmax\n",
    "def accuracy(X, y) -> float:\n",
    "    x = np.argmax(X, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    return np.mean(x == y)\n",
    "\n",
    "print(accuracy(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More infomation on networks, and other things that you can do"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
