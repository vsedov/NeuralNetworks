{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Back Prop\n",
    "With the given data that you have at this moment, there are two things that you\n",
    "should know about , when you are dealing with the given values of the relu, and\n",
    "you want to minimise the output of that , we are taking those values as one\n",
    "batch, this is show below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from pprintpp import pprint as pp\n",
    "dvalues = np.array([[1,1,1],[2,2,2],[3,3,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values here, are the gradients from the next layer that have been passed\n",
    "down , its rather easy to see how that works , in this example , ive made it\n",
    "easy that its only 111 ,222 and so on, but the point for this stands .\n",
    "\n",
    "the second values is the inputs , these are teh sample data , and not hte\n",
    "gradient data that we get , or the other values that have been pased from the\n",
    "next given layer ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1. ,  2. ,  3. ,  2.5],\n",
      "       [ 2. ,  5. , -1. ,  2. ],\n",
      "       [-1.5,  2.7,  3.3, -0.8]])\n",
      "array([[1, 1, 1],\n",
      "       [2, 2, 2],\n",
      "       [3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[1,2,3,2.5],[2,5,-1,2],[-1.5,2.7,3.3,-0.8]])\n",
    "pp(inputs)\n",
    "pp(dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given set of data that we have, nothing more to that  - just data that we are\n",
    "passing down ,\n",
    "\n",
    "the dvalues arefixed , as they are teh gradient values , from the next layer ,\n",
    "to note this values would tend to stay or lean towards just being one\n",
    "\n",
    "We would then have to pass through the weights, : say that we have 3 sets of\n",
    "weights : and such that there is one for each given neuron , and because there\n",
    "are four values , we woud have to transpose tese values such that the shape\n",
    "matches , shown within, linear algebriac combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.2 ,  0.5 , -0.26],\n",
      "       [ 0.8 , -0.91, -0.27],\n",
      "       [-0.5 ,  0.26,  0.17],\n",
      "       [ 1.  , -0.5 ,  0.87]])\n"
     ]
    }
   ],
   "source": [
    "weights = np.array(\n",
    "    [[0.2, 0.8, -0.5, 1], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]\n",
    ").T\n",
    "pp(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to define the bias and the layer outputs, with the outsputs, you\n",
    "should know that it is defined as teh dot product , so that would of been easy\n",
    "and we are just quickly defining the relu option  : given code shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    array([[ 4.8  ,  1.21 ,  2.385],\n",
      "           [ 8.9  , -1.81 ,  0.2  ],\n",
      "           [ 1.41 ,  1.051,  0.026]]),\n",
      "    array([[4.8  , 1.21 , 2.385],\n",
      "           [8.9  , 0.   , 0.2  ],\n",
      "           [1.41 , 1.051, 0.026]]),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "layer_outputs = np.dot(inputs, weights) + biases\n",
    "relu_outputs = np.maximum(0,layer_outputs)\n",
    "\n",
    "pp([layer_outputs, relu_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , the back prop operations would start, or the given values when we are\n",
    "doing the backwards passs, which ofcours eis the most important pass of the\n",
    "system\n",
    "\n",
    "\n",
    "Relu activation, simulates teh dx with respects to teh input value , from teh\n",
    "next layer passed to teh current layer, this is done during back prop:\n",
    "Given code is show below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[4.8  , 1.21 , 2.385],\n",
      "       [8.9  , 0.   , 0.2  ],\n",
      "       [1.41 , 1.051, 0.026]])\n"
     ]
    }
   ],
   "source": [
    "drelu = relu_outputs.copy()\n",
    "drelu[relu_outputs <= 0 ] = 0 \n",
    "pp(drelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to deal with teh dense layer, this again is quite straight forward\n",
    "as this is just using, the dx values that we had  before, but now we are\n",
    "properly doing the given backwards pass :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu : \n",
      " [[4.8   1.21  2.385]\n",
      " [8.9   0.    0.2  ]\n",
      " [1.41  1.051 0.026]] \n",
      "\n",
      "weights\n",
      " [[ 0.2   0.5  -0.26]\n",
      " [ 0.8  -0.91 -0.27]\n",
      " [-0.5   0.26  0.17]\n",
      " [ 1.   -0.5   0.87]] \n",
      "\n",
      "Weights.T \n",
      " [[ 0.2   0.8  -0.5   1.  ]\n",
      " [ 0.5  -0.91  0.26 -0.5 ]\n",
      " [-0.26 -0.27  0.17  0.87]] \n",
      "\n",
      "Inputs \n",
      " [[ 1.   2.   3.   2.5]\n",
      " [ 2.   5.  -1.   2. ]\n",
      " [-1.5  2.7  3.3 -0.8]] \n",
      "\n",
      "Inputs.T \n",
      " [[ 1.   2.  -1.5]\n",
      " [ 2.   5.   2.7]\n",
      " [ 3.  -1.   3.3]\n",
      " [ 2.5  2.  -0.8]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"drelu : \\n\",drelu,\"\\n\")\n",
    "print(\"weights\\n\", weights,\"\\n\")\n",
    "print(\"Weights.T \\n\",weights.T,\"\\n\")\n",
    "print(\"Inputs \\n\",inputs,\"\\n\")\n",
    "print(\"Inputs.T \\n\",inputs.T,\"\\n\")\n",
    "dinputs = np.dot(drelu, weights.T)\n",
    "dweights = np.dot(inputs.T, drelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the final update of the given code, we would have tofix the dx of inputs\n",
    "and dx of weights ,, in this case what we are doing is for the inputs , dxinputs\n",
    "= drelu - all values greater than zero, multiply by the given weights, and this\n",
    "is quite important  becase partial dx of inputs is teh weights\n",
    "and the partial dx of weights is  the inputs , so when we are calculating the\n",
    "given values for the dweights , the shape of the inputs woul dhave to be the\n",
    "same as teh drelu and once we have that , we should get the folllowing code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [
      "inputs"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    array([[ 0.2 ,  0.5 , -0.26],\n",
      "           [ 0.8 , -0.91, -0.27],\n",
      "           [-0.5 ,  0.26,  0.17],\n",
      "           [ 1.  , -0.5 ,  0.87]]),\n",
      "    array([[ 1. ,  2. ,  3. ,  2.5],\n",
      "           [ 2. ,  5. , -1. ,  2. ],\n",
      "           [-1.5,  2.7,  3.3, -0.8]]),\n",
      "]\n",
      "[\n",
      "    'Updated Param',\n",
      "    array([[ 0.179515 ,  0.5003665, -0.262746 ],\n",
      "           [ 0.742093 , -0.9152577, -0.2758402],\n",
      "           [-0.510153 ,  0.2529017,  0.1629592],\n",
      "           [ 0.971328 , -0.5021842,  0.8636583]]),\n",
      "    array([[1.98489 , 2.997739, 0.497389]]),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "pp([weights, inputs])\n",
    "\n",
    "dbiases = np.sum(drelu,axis = 0 , keepdims=True)\n",
    "\n",
    "weights += -0.001 *dweights\n",
    "biases += -0.001 * dbiases\n",
    "\n",
    "pp([\"Updated Param\",weights, biases])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
